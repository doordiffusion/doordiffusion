<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Diffusion-based Door Opening and Traversal Policy</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <!-- Header -->
    <header class="header">
        <div class="container">
            <div class="nav">
                <div class="nav-brand">
                    <a href="#top" class="nav-brand-link">
                        <h1>DoorDiffusion</h1>
                    </a>
                </div>
                <nav class="nav-links">
                    <a href="#abstract">Abstract</a>
                    <a href="#method">Method</a>
                    <a href="#results">Results</a>
                    <a href="#conclusion">Conclusion</a>
                </nav>
            </div>
        </div>
    </header>

    <!-- Hero Section -->
    <section id="top" class="hero">
        <div class="container">
            <div class="hero-content">
            <h1 class="hero-title">Diffusion Policy for Coordinated Control of a Nonholonomic Mobile Base and Dual Arms in Door Opening and Passing</h1>
                <div class="hero-meta">
                    <span>Author 1, Author 2, Author 3, ...</span>
                </div>
            </div>
            
            <div class="hero-video">
                <div class="video-placeholder">
                    <div class="play-button">
                        <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                            <path d="M8 5v14l11-7z"/>
                        </svg>
                    </div>
                    <p>Demo Video</p>
                    <span>Click to play demonstration</span>
                </div>
            </div>
            
        </div>
    </section>

    <!-- Abstract Section -->
    <section id="abstract" class="abstract">
        <div class="container">
            <h2 class="section-title">Abstract</h2>
            <div class="abstract-content">
                <p>
                    Opening heavy, self-closing doorsâ€”especially those that require pullingâ€”remains a long-standing challenge in robotics. Humans naturally employ both arms in a dexterous mannerâ€”rotating the handle, widening the gap, holding the door, switching arms when needed, and moving through while maintaining clearance. To replicate such behaviors, a robot must perform a long sequence of motions spanning multiple stages and interactions with different parts of the door. Traditional approaches rely on state machines that transition between manually defined stages (e.g., pulling after the knob is rotated, passing after the gap is sufficiently wide). While intuitive, these methods lack robustness, as hand-crafted trajectories fail to generalize to the diversity of real-world conditions without extensive engineering effort. Recent advances in imitation learning offer a scalable alternative, yet no existing visual-action model has demonstrated simultaneous coordination of a nonholonomic base and dual arms for the complete door opening and passing task. In this paper, we tackle this complex, highly constrained problem using a diffusion-based visuomotor control policy. Our results demonstrate that a single end-to-end policy can be learned to execute long-horizon tasks requiring tight coordination between manipulation and locomotion. The resulting policy not only achieves a high success rate in opening and traversing damped pull doors but also demonstrates strong robustness to external disturbancesâ€”capabilities that are difficult to realize with traditional methods.
                </p>
                <img src="images/hdsf.png" alt="Diffusion-based Door Opening and Traversal Policy" class="abstract-image">
            </div>
        </div>
    </section>

    <!-- Method Section -->
    <section id="method" class="method">
        <div class="container">
            <h2 class="section-title">Method</h2>
            
            <!-- A. Data Collection -->
            <div class="data-collection">
                <h3>I. Data Collection</h3>
                
                <img src="images/dataCollect.png" alt="Hardware Specifications and Data Collection" class="teleop-main-image">
                
                <div class="data-collection-content">
                    <div class="data-collection-text">
                        <div class="simulation-section">
                            <div class="simulation-text">
                                <h4>Simulation</h4>
                                <p>
                                    To collect data in simulation, we use a state-based controller that combines inverse kinematics (IK) for door manipulation with model predictive control (MPC) for base motion. To better approximate human-like behavior, rather than directly performing joint-level control with the IK solution, we developed a scheduler that gradually guides the robot in the task-space toward the desired joint configuration found by the IK. To enhance realism and variability, we randomize lighting conditions as well as door and handle colors across episodes, thereby reflecting the natural diversity of real-world environments.
                                </p>
                                
                                <p>
                                    In both hardware and simulation, we generated 100 demonstration trajectories. To prevent the policies from overfitting to a single motion pattern, we additionally randomize the robot's initial base pose (dx, dy, and dyaw). The robot base is placed at a random lateral distance of 0.90Â±0.03m with longitudinal offset of Â±0.03m, and yaw of Â±1.00 rad.
                                </p>
                            </div>
                            
                            <div class="simulation-image">
                                <img src="images/ik_MPC.png" alt="IK and MPC Controller" class="ik-mpc-image">
                            </div>
                        </div>
                        
                        <h4>Hardware</h4>
                        <p>
                            For collecting expert demonstrations on hardware, we use the teleoperation kit provided by Realman, which has the same joint configuration as the target robot. We collect image data from three independent cameras capturing RGB images of shape 180Ã—240 pixels. During teleoperation, the robot's joint positions and images from all three cameras are recorded as the state, while the joint positions commanded through the teleoperation kit are stored as actions.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Policy and Hyperparameters Section -->
    <section id="policy" class="policy">
        <div class="container">
            <h3>II. Policy and Hyperparameters</h3>
            
            <div class="policy-content">
                <div class="policy-text">
                    <p>
                        Our diffusion-based policy architecture consists of three independent ResNet-18 encoders for multi-view perception and a 1D U-Net with FiLM conditioning for action generation. The policy operates with a prediction horizon of 16 steps and generates action sequences of length 8, using stacked observations from the last 3 timesteps.
                    </p>
                    
                    <p>
                        The diffusion process uses 100 forward diffusion steps during training, which are reduced to 10 steps during inference for computational efficiency. This approach enables the policy to learn smooth, coordinated actions while maintaining robustness to the complex dynamics of door opening and traversal tasks.
                    </p>
                </div>
                
                <div class="policy-images">
                    <img src="images/model.png" alt="Policy Architecture" class="model-image">
                    <img src="images/hyperparameters.png" alt="Hyperparameters Table" class="hyperparameters-image">
                </div>
            </div>
        </div>
    </section>

    <!-- Results Section -->
    <section id="results" class="results">
        <div class="container">
            <h2 class="section-title">Results</h2>
            
            <!-- Simulation Results -->
            <div class="results-section">
                <h3>Simulation</h3>
                <div class="results-content">
                    <div class="results-text">
                        <p>
                            We successfully trained a diffusion policy that enables a mobile manipulator to open and traverse a damped pull door 
                            using only visual and proprioceptive inputs. The policy learns a long-horizon trajectory that integrates multiple 
                            coordinated skills, including reaching for the handle, twisting it, pulling the door, coordinating both arms, and 
                            synchronizing manipulation with locomotion.
                        </p>
                        <div class="results-highlights">
                            <div class="highlight">
                                <div class="highlight-icon">âœ“</div>
                                <div class="highlight-text">
                                    <strong>100% Success Rate</strong>
                                    <span>With separate encoders vs 0% with unified encoder</span>
                                </div>
                            </div>
                            <div class="highlight">
                                <div class="highlight-icon">âš¡</div>
                                <div class="highlight-text">
                                    <strong>Optimized Inference</strong>
                                    <span>Reduced from 100 to 10 denoising steps without performance loss</span>
                                </div>
                            </div>
                            <div class="highlight">
                                <div class="highlight-icon">ðŸŽ¯</div>
                                <div class="highlight-text">
                                    <strong>Superior Performance</strong>
                                    <span>Outperformed ACT (8/10) and SmolVLA (0/10) baselines</span>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="results-visual">
                        <img src="images/actionOverTime.png" alt="Policy Deployment Comparison" class="deployment-image">
                        <p class="figure-caption">Policy deployment comparison between simulation and hardware. Shows synchronized state and action trajectories with colored bands indicating key behavioral stages.</p>
                    </div>
                </div>
            </div>

            <!-- Hardware Results -->
            <div class="results-section">
                <h3>Hardware</h3>
                <div class="hardware-results">
                    <div class="hardware-text">
                        <p>
                            The policy successfully executed reliable door-opening behaviors with bimanual coordination on real hardware. 
                            Notably, the learned policy demonstrated robustness to disturbances: when the door was manually re-closed during 
                            execution, the policy responded by halting further extension and re-initiating the opening sequence.
                        </p>
                    </div>
                    <div class="hardware-visual">
                        <img src="images/disturbance.png" alt="Disturbance Recovery Sequence" class="disturbance-image">
                        <p class="figure-caption">Policy under external disturbance during door opening. Shows the robot's ability to detect changes and re-initiate the opening sequence when the door is manually re-closed.</p>
                    </div>
                </div>
            </div>

            <!-- Ablation Study -->
            <div class="ablation-study">
                <h3>Ablation Study</h3>
                <img src="images/simresults.png" alt="Simulation Results Table" class="results-table-image">
                <p class="figure-caption">Simulation results comparing different policy configurations and baselines. Our diffusion policy with separate encoders achieves 100% success rate.</p>
            </div>
        </div>
    </section>

    <!-- Conclusion Section -->
    <section id="conclusion" class="conclusion">
        <div class="container">
            <h2 class="section-title">Conclusion</h2>
            <div class="conclusion-content">
                <p>
                    Our study demonstrates that diffusion-based visuomotor policies can achieve reliable performance on the challenging 
                    task of opening and traversing damped pull doors using a dual-arm mobile manipulator. Unlike prior approaches that 
                    rely on state machines or heavily engineered perception pipelines, our method learns a unified policy that integrates 
                    perception, manipulation, and base coordination directly from demonstration data. The results show that diffusion 
                    policies not only generate long-horizon trajectories but also exhibit robustness to disturbances and environmental 
                    variability, a critical capability for real-world deployment.
                </p>
            </div>
        </div>
    </section>


    <script>
        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });

        // Add scroll effect to header
        window.addEventListener('scroll', function() {
            const header = document.querySelector('.header');
            if (window.scrollY > 100) {
                header.classList.add('scrolled');
            } else {
                header.classList.remove('scrolled');
            }
        });
    </script>
</body>
</html>
